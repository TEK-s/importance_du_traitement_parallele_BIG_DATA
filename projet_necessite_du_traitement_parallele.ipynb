{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "166d3c25",
   "metadata": {},
   "source": [
    "#  Montrer la garantie du passage à l’échelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cad6602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from time import time\n",
    "import numpy as np\n",
    "#sc.stop()\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName(\"adult_data\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7afd63bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = spark.sparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d482dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DIM-P4-6600-09.laboratoire.uqac.ca:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>adult_data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x201eb70e828>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23e5b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"adult1.data\"\n",
    "data = spark.read.csv(path, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9c73ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2565336"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.sample(0.3)\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec76b7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: double (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: double (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: double (nullable = true)\n",
      " |-- _c11: double (nullable = true)\n",
      " |-- _c12: double (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36b461f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names=[('_c0','age'),('_c1','workclass'),('_c2','fnlwgt'),('_c3','education'),('_c4','education_num'),('_c5','marital_status'),('_c6','occupation'),('_c7','relationship'),('_c8','race'),('_c9','sex'),('_c10','capital_gain'),('_c11','capital_loss'),('_c12','hours_per_week'),('_c13','native_country'),('_c14','classe')]\n",
    "for old_name,new_name in new_names:\n",
    "    data=data.withColumnRenamed(old_name,new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f678f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: double (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: double (nullable = true)\n",
      " |-- marital_status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: double (nullable = true)\n",
      " |-- capital_loss: double (nullable = true)\n",
      " |-- hours_per_week: double (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- classe: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28ab8b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(\"native_country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "217aaa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer=StringIndexer().setInputCols([\"workclass\",\"education\",\"marital_status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"classe\"]).setOutputCols([\"workclass_indexed\",\"education_indexed\",\"marital_status_indexed\",\"occupation_indexed\",\"relationship_indexed\",\"race_indexed\",\"sex_indexed\",\"classe_indexed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b769af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed=indexer.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38604432",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembleur = VectorAssembler()\\\n",
    "    .setInputCols(['age','workclass_indexed','fnlwgt','education_indexed','education_num','marital_status_indexed','occupation_indexed','relationship_indexed','race_indexed','sex_indexed'])\\\n",
    "    .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb5d63e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=assembleur.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ee1e847",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data=output.select(\"features\",\"classe_indexed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93d87305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|            features|classe_indexed|\n",
      "+--------------------+--------------+\n",
      "|[32.0,0.0,205019....|           0.0|\n",
      "|[32.0,0.0,186824....|           0.0|\n",
      "+--------------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_data.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530cdab6",
   "metadata": {},
   "source": [
    "# Résultat de l'expérimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2b17408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier predict sample 5.0% in 284.878 seconds\n",
      "Classifier predict sample 10.0% in 273.202 seconds\n",
      "Classifier predict sample 15.000000000000002% in 281.397 seconds\n",
      "Classifier predict sample 20.0% in 265.693 seconds\n",
      "Classifier predict sample 25.0% in 275.886 seconds\n",
      "Classifier predict sample 30.0% in 275.695 seconds\n",
      "Classifier predict sample 35.0% in 279.54 seconds\n",
      "Classifier predict sample 40.0% in 283.064 seconds\n",
      "Classifier predict sample 45.0% in 368.933 seconds\n",
      "Classifier predict sample 50.0% in 331.279 seconds\n",
      "Classifier predict sample 55.00000000000001% in 342.574 seconds\n",
      "Classifier predict sample 60.00000000000001% in 354.937 seconds\n",
      "Classifier predict sample 65.00000000000001% in 343.872 seconds\n",
      "Classifier predict sample 70.0% in 348.28 seconds\n",
      "Classifier predict sample 75.00000000000001% in 357.328 seconds\n",
      "Classifier predict sample 80.0% in 367.078 seconds\n",
      "Classifier predict sample 85.00000000000001% in 376.934 seconds\n",
      "Classifier predict sample 90.00000000000001% in 383.415 seconds\n",
      "Classifier predict sample 95.0% in 440.342 seconds\n",
      "Classifier predict sample 100.0% in 446.344 seconds\n"
     ]
    }
   ],
   "source": [
    "time_prediction=[]\n",
    "classifier = DecisionTreeClassifier(labelCol=\"classe_indexed\", featuresCol=\"features\")\n",
    "# add empty parameter grid \n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(classifier.maxDepth, [2,5,10,15,20,25]) # max depth parameter \n",
    "             .build())\n",
    "    \n",
    "# create evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"classe_indexed\")\n",
    "\n",
    "# create cross validation object\n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5) \n",
    "for i in np.arange(0.05,1.05,0.05):\n",
    "    sample=model_data.sample(i,123)\n",
    "    (trainingData, testData) = sample.randomSplit([0.7, 0.3])\n",
    "\n",
    "    t0 = time()\n",
    "    # run cross-validation, and choose the best set of parameters.\n",
    "    Model = crossval.fit(trainingData)\n",
    "    #tt = time() - t0\n",
    "\n",
    "    #print (\"Classifier trained in {} seconds\".format(round(tt,3)))\n",
    "    #t0 = time()\n",
    "\n",
    "    # make predictions on test data \n",
    "    predictions = Model.transform(testData)\n",
    "\n",
    "    tt = time() - t0\n",
    "    time_prediction.append(tt)\n",
    "    b=i*100\n",
    "    print (\"Classifier predict sample {}% in {} seconds\".format(b,round(tt,3)))\n",
    "    #acc = evaluator.evaluate(predictions)\n",
    "    #print(\"Prediction Accuracy: \", predictions)\n",
    "\n",
    "    #y_pred=predictions.select(\"prediction\").collect()\n",
    "    #y_orig=predictions.select(\"classe_indexed\").collect()\n",
    "\n",
    "    #cm = confusion_matrix(y_orig, y_pred)\n",
    "    #print(\"Confusion Matrix:\", cm)\n",
    "    #print('Accuracy:', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ec037fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "T=[]\n",
    "S=np.array(time_prediction)\n",
    "for i in np.arange(0.05,1.05,0.05):\n",
    "    b=i/0.05\n",
    "    T.append(b)\n",
    "#S=np.array([34.505,33.306,35.846,35.708,38.136,37.237,43.079,40.751,43.77,41.608,45.652,43.326,46.551,46.081,48.126,48.684,50.077,49.924,51.885]) \n",
    "time_prediction=S/min(time_prediction)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(T,T)\n",
    "plt.plot(T,time_prediction)\n",
    "plt.ylabel('some numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "71012197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  DataFrame[features: vector, classe_indexed: double, rawPrediction: vector, probability: vector, prediction: double]\n",
      "Confusion Matrix:\n",
      "[[6271  689]\n",
      " [ 813 1394]]\n",
      "Accuracy: 0.8345067795666556\n"
     ]
    }
   ],
   "source": [
    "acc = evaluator.evaluate(predictions)\n",
    "print(\"Prediction Accuracy: \", predictions)\n",
    "\n",
    "y_pred=predictions.select(\"prediction\").collect()\n",
    "y_orig=predictions.select(\"classe_indexed\").collect()\n",
    "\n",
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print('Accuracy:', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac10cfe9",
   "metadata": {},
   "source": [
    "# exprimentation avec 2 coeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "571c5e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a95e11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from time import time\n",
    "import numpy as np\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"test_avec_2coeur\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37388385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = spark.sparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa2c6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "path=\"adult2.data\"\n",
    "data = spark.read.csv(path, inferSchema=True)\n",
    "new_names=[('_c0','age'),('_c1','workclass'),('_c2','fnlwgt'),('_c3','education'),('_c4','education_num'),('_c5','marital_status'),('_c6','occupation'),('_c7','relationship'),('_c8','race'),('_c9','sex'),('_c10','capital_gain'),('_c11','capital_loss'),('_c12','hours_per_week'),('_c13','native_country'),('_c14','classe')]\n",
    "for old_name,new_name in new_names:\n",
    "    data=data.withColumnRenamed(old_name,new_name)\n",
    "data=data.drop(\"native_country\")\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "workclass_indexer = StringIndexer().setInputCol(\"workclass\").setOutputCol(\"workclass_indexed\")\n",
    "education_indexer = StringIndexer().setInputCol(\"education\").setOutputCol(\"education_indexed\")\n",
    "marital_status_indexer = StringIndexer().setInputCol(\"marital_status\").setOutputCol(\"marital_status_indexed\")\n",
    "occupation_indexer = StringIndexer().setInputCol(\"occupation\").setOutputCol(\"occupation_indexed\")\n",
    "relationship_indexer = StringIndexer().setInputCol(\"relationship\").setOutputCol(\"relationship_indexed\")\n",
    "race_indexer = StringIndexer().setInputCol(\"race\").setOutputCol(\"race_indexed\")\n",
    "sex_indexer = StringIndexer().setInputCol(\"sex\").setOutputCol(\"sex_indexed\")\n",
    "classe_indexer = StringIndexer().setInputCol(\"classe\").setOutputCol(\"classe_indexed\")\n",
    "\n",
    "numCol = [\"age\",'fnlwgt', 'education_num' ]\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembleur = VectorAssembler()\\\n",
    "    .setInputCols(['workclass_indexed','education_indexed','marital_status_indexed','occupation_indexed','relationship_indexed','race_indexed','sex_indexed'] + numCol)\\\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#définition du pipeline\n",
    "transformationPipeline = Pipeline()\\\n",
    "                        .setStages([classe_indexer, sex_indexer, race_indexer, relationship_indexer ,occupation_indexer, marital_status_indexer, education_indexer, workclass_indexer,  assembleur])\n",
    "\n",
    "\n",
    "#la fonction fit pour rencenser les valeurs possibles des index pour chaque variable à partir du dataframe \n",
    "fittedPipeline = transformationPipeline.fit(data)\n",
    "\n",
    "#appliquer les résultats de la fonction fit pour transformer le DataFrame en ajoutnat les nouvelles colonnes.\n",
    "#on obtien une nouvelle dataframe\n",
    "transformedTraining = fittedPipeline.transform(data)\n",
    "model_data=transformedTraining.select(\"features\",\"classe_indexed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f5f3920",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData=model_data.sample(0.7,123)\n",
    "testData=model_data.sample(0.3,123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7464e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  DataFrame[features: vector, classe_indexed: double, rawPrediction: vector, probability: vector, prediction: double]\n",
      "Confusion Matrix:\n",
      "[[6850  626]\n",
      " [ 809 1591]]\n",
      "Accuracy: 0.8526970939481168\n",
      "Classifier trained in 41.718 seconds\n"
     ]
    }
   ],
   "source": [
    "time_prediction=[]\n",
    "classifier = DecisionTreeClassifier(labelCol=\"classe_indexed\", featuresCol=\"features\")\n",
    "# add empty parameter grid \n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(classifier.maxDepth, [2,5,10,15,20,25]) # max depth parameter \n",
    "             .build())\n",
    "    \n",
    "# create evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"classe_indexed\")\n",
    "\n",
    "# create cross validation object\n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5) \n",
    "\n",
    "#model_data=model_data.sample(0.1,123)\n",
    "#(trainingData, testData) = model_data.randomSplit([0.7, 0.3])\n",
    "\n",
    "t0 = time()\n",
    "# run cross-validation, and choose the best set of parameters.\n",
    "Model = crossval.fit(trainingData)\n",
    "# make predictions on test data \n",
    "predictions = Model.transform(testData)\n",
    "tt = time() - t0\n",
    "acc = evaluator.evaluate(predictions)\n",
    "print(\"Prediction Accuracy: \", predictions)\n",
    "\n",
    "y_pred=predictions.select(\"prediction\").collect()\n",
    "y_orig=predictions.select(\"classe_indexed\").collect()\n",
    "\n",
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print('Accuracy:', evaluator.evaluate(predictions))\n",
    "print (\"Classifier trained in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8bb21",
   "metadata": {},
   "source": [
    "# 4 coeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d071b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b2f710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from time import time\n",
    "import numpy as np\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[4]\")\\\n",
    "        .appName(\"test_avec_4coeur\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27a06c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = spark.sparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a6dee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "path=\"adult2.data\"\n",
    "data = spark.read.csv(path, inferSchema=True)\n",
    "new_names=[('_c0','age'),('_c1','workclass'),('_c2','fnlwgt'),('_c3','education'),('_c4','education_num'),('_c5','marital_status'),('_c6','occupation'),('_c7','relationship'),('_c8','race'),('_c9','sex'),('_c10','capital_gain'),('_c11','capital_loss'),('_c12','hours_per_week'),('_c13','native_country'),('_c14','classe')]\n",
    "for old_name,new_name in new_names:\n",
    "    data=data.withColumnRenamed(old_name,new_name)\n",
    "data=data.drop(\"native_country\")\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "workclass_indexer = StringIndexer().setInputCol(\"workclass\").setOutputCol(\"workclass_indexed\")\n",
    "education_indexer = StringIndexer().setInputCol(\"education\").setOutputCol(\"education_indexed\")\n",
    "marital_status_indexer = StringIndexer().setInputCol(\"marital_status\").setOutputCol(\"marital_status_indexed\")\n",
    "occupation_indexer = StringIndexer().setInputCol(\"occupation\").setOutputCol(\"occupation_indexed\")\n",
    "relationship_indexer = StringIndexer().setInputCol(\"relationship\").setOutputCol(\"relationship_indexed\")\n",
    "race_indexer = StringIndexer().setInputCol(\"race\").setOutputCol(\"race_indexed\")\n",
    "sex_indexer = StringIndexer().setInputCol(\"sex\").setOutputCol(\"sex_indexed\")\n",
    "classe_indexer = StringIndexer().setInputCol(\"classe\").setOutputCol(\"classe_indexed\")\n",
    "\n",
    "numCol = [\"age\",'fnlwgt', 'education_num' ]\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembleur = VectorAssembler()\\\n",
    "    .setInputCols(['workclass_indexed','education_indexed','marital_status_indexed','occupation_indexed','relationship_indexed','race_indexed','sex_indexed'] + numCol)\\\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#définition du pipeline\n",
    "transformationPipeline = Pipeline()\\\n",
    "                        .setStages([classe_indexer, sex_indexer, race_indexer, relationship_indexer ,occupation_indexer, marital_status_indexer, education_indexer, workclass_indexer,  assembleur])\n",
    "\n",
    "\n",
    "#la fonction fit pour rencenser les valeurs possibles des index pour chaque variable à partir du dataframe \n",
    "fittedPipeline = transformationPipeline.fit(data)\n",
    "\n",
    "#appliquer les résultats de la fonction fit pour transformer le DataFrame en ajoutnat les nouvelles colonnes.\n",
    "#on obtien une nouvelle dataframe\n",
    "transformedTraining = fittedPipeline.transform(data)\n",
    "model_data=transformedTraining.select(\"features\",\"classe_indexed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "202b8a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData=model_data.sample(0.7,123)\n",
    "testData=model_data.sample(0.3,123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3f5f3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  DataFrame[features: vector, classe_indexed: double, rawPrediction: vector, probability: vector, prediction: double]\n",
      "Confusion Matrix:\n",
      "[[6850  626]\n",
      " [ 809 1591]]\n",
      "Accuracy: 0.8526970939481168\n",
      "Classifier trained in 40.095 seconds\n"
     ]
    }
   ],
   "source": [
    "time_prediction=[]\n",
    "classifier = DecisionTreeClassifier(labelCol=\"classe_indexed\", featuresCol=\"features\")\n",
    "# add empty parameter grid \n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(classifier.maxDepth, [2,5,10,15,20,25]) # max depth parameter \n",
    "             .build())\n",
    "    \n",
    "# create evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"classe_indexed\")\n",
    "\n",
    "# create cross validation object\n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5) \n",
    "\n",
    "#model_data=model_data.sample(0.1,123)\n",
    "#(trainingData, testData) = model_data.randomSplit([0.7, 0.3])\n",
    "\n",
    "t0 = time()\n",
    "# run cross-validation, and choose the best set of parameters.\n",
    "Model = crossval.fit(trainingData)\n",
    "# make predictions on test data \n",
    "predictions = Model.transform(testData)\n",
    "tt = time() - t0\n",
    "acc = evaluator.evaluate(predictions)\n",
    "print(\"Prediction Accuracy: \", predictions)\n",
    "\n",
    "y_pred=predictions.select(\"prediction\").collect()\n",
    "y_orig=predictions.select(\"classe_indexed\").collect()\n",
    "\n",
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print('Accuracy:', evaluator.evaluate(predictions))\n",
    "print (\"Classifier trained in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce384da",
   "metadata": {},
   "source": [
    "# 6 coeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6adeef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44356865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from time import time\n",
    "import numpy as np\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[6]\")\\\n",
    "        .appName(\"test_avec_6coeur\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "path=\"adult2.data\"\n",
    "data = spark.read.csv(path, inferSchema=True)\n",
    "new_names=[('_c0','age'),('_c1','workclass'),('_c2','fnlwgt'),('_c3','education'),('_c4','education_num'),('_c5','marital_status'),('_c6','occupation'),('_c7','relationship'),('_c8','race'),('_c9','sex'),('_c10','capital_gain'),('_c11','capital_loss'),('_c12','hours_per_week'),('_c13','native_country'),('_c14','classe')]\n",
    "for old_name,new_name in new_names:\n",
    "    data=data.withColumnRenamed(old_name,new_name)\n",
    "data=data.drop(\"native_country\")\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "workclass_indexer = StringIndexer().setInputCol(\"workclass\").setOutputCol(\"workclass_indexed\")\n",
    "education_indexer = StringIndexer().setInputCol(\"education\").setOutputCol(\"education_indexed\")\n",
    "marital_status_indexer = StringIndexer().setInputCol(\"marital_status\").setOutputCol(\"marital_status_indexed\")\n",
    "occupation_indexer = StringIndexer().setInputCol(\"occupation\").setOutputCol(\"occupation_indexed\")\n",
    "relationship_indexer = StringIndexer().setInputCol(\"relationship\").setOutputCol(\"relationship_indexed\")\n",
    "race_indexer = StringIndexer().setInputCol(\"race\").setOutputCol(\"race_indexed\")\n",
    "sex_indexer = StringIndexer().setInputCol(\"sex\").setOutputCol(\"sex_indexed\")\n",
    "classe_indexer = StringIndexer().setInputCol(\"classe\").setOutputCol(\"classe_indexed\")\n",
    "\n",
    "numCol = [\"age\",'fnlwgt', 'education_num' ]\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembleur = VectorAssembler()\\\n",
    "    .setInputCols(['workclass_indexed','education_indexed','marital_status_indexed','occupation_indexed','relationship_indexed','race_indexed','sex_indexed'] + numCol)\\\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#définition du pipeline\n",
    "transformationPipeline = Pipeline()\\\n",
    "                        .setStages([classe_indexer, sex_indexer, race_indexer, relationship_indexer ,occupation_indexer, marital_status_indexer, education_indexer, workclass_indexer,  assembleur])\n",
    "\n",
    "\n",
    "#la fonction fit pour rencenser les valeurs possibles des index pour chaque variable à partir du dataframe \n",
    "fittedPipeline = transformationPipeline.fit(data)\n",
    "\n",
    "#appliquer les résultats de la fonction fit pour transformer le DataFrame en ajoutnat les nouvelles colonnes.\n",
    "#on obtien une nouvelle dataframe\n",
    "transformedTraining = fittedPipeline.transform(data)\n",
    "model_data=transformedTraining.select(\"features\",\"classe_indexed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd50952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData=model_data.sample(0.7,123)\n",
    "testData=model_data.sample(0.3,123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f9f4b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  DataFrame[features: vector, classe_indexed: double, rawPrediction: vector, probability: vector, prediction: double]\n",
      "Confusion Matrix:\n",
      "[[6850  626]\n",
      " [ 809 1591]]\n",
      "Accuracy: 0.8526970939481168\n",
      "Classifier trained in 39.699 seconds\n"
     ]
    }
   ],
   "source": [
    "time_prediction=[]\n",
    "classifier = DecisionTreeClassifier(labelCol=\"classe_indexed\", featuresCol=\"features\")\n",
    "# add empty parameter grid \n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(classifier.maxDepth, [2,5,10,15,20,25]) # max depth parameter \n",
    "             .build())\n",
    "    \n",
    "# create evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"classe_indexed\")\n",
    "\n",
    "# create cross validation object\n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5) \n",
    "\n",
    "#model_data=model_data.sample(0.1,123)\n",
    "#(trainingData, testData) = model_data.randomSplit([0.7, 0.3])\n",
    "\n",
    "t0 = time()\n",
    "# run cross-validation, and choose the best set of parameters.\n",
    "Model = crossval.fit(trainingData)\n",
    "# make predictions on test data \n",
    "predictions = Model.transform(testData)\n",
    "tt = time() - t0\n",
    "acc = evaluator.evaluate(predictions)\n",
    "print(\"Prediction Accuracy: \", predictions)\n",
    "\n",
    "y_pred=predictions.select(\"prediction\").collect()\n",
    "y_orig=predictions.select(\"classe_indexed\").collect()\n",
    "\n",
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print('Accuracy:', evaluator.evaluate(predictions))\n",
    "print (\"Classifier trained in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aec34a2",
   "metadata": {},
   "source": [
    "# 8 coeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a993f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "090c3566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from time import time\n",
    "import numpy as np\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[8]\")\\\n",
    "        .appName(\"test_avec_8coeur\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "path=\"adult2.data\"\n",
    "data = spark.read.csv(path, inferSchema=True)\n",
    "new_names=[('_c0','age'),('_c1','workclass'),('_c2','fnlwgt'),('_c3','education'),('_c4','education_num'),('_c5','marital_status'),('_c6','occupation'),('_c7','relationship'),('_c8','race'),('_c9','sex'),('_c10','capital_gain'),('_c11','capital_loss'),('_c12','hours_per_week'),('_c13','native_country'),('_c14','classe')]\n",
    "for old_name,new_name in new_names:\n",
    "    data=data.withColumnRenamed(old_name,new_name)\n",
    "data=data.drop(\"native_country\")\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "workclass_indexer = StringIndexer().setInputCol(\"workclass\").setOutputCol(\"workclass_indexed\")\n",
    "education_indexer = StringIndexer().setInputCol(\"education\").setOutputCol(\"education_indexed\")\n",
    "marital_status_indexer = StringIndexer().setInputCol(\"marital_status\").setOutputCol(\"marital_status_indexed\")\n",
    "occupation_indexer = StringIndexer().setInputCol(\"occupation\").setOutputCol(\"occupation_indexed\")\n",
    "relationship_indexer = StringIndexer().setInputCol(\"relationship\").setOutputCol(\"relationship_indexed\")\n",
    "race_indexer = StringIndexer().setInputCol(\"race\").setOutputCol(\"race_indexed\")\n",
    "sex_indexer = StringIndexer().setInputCol(\"sex\").setOutputCol(\"sex_indexed\")\n",
    "classe_indexer = StringIndexer().setInputCol(\"classe\").setOutputCol(\"classe_indexed\")\n",
    "\n",
    "numCol = [\"age\",'fnlwgt', 'education_num' ]\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembleur = VectorAssembler()\\\n",
    "    .setInputCols(['workclass_indexed','education_indexed','marital_status_indexed','occupation_indexed','relationship_indexed','race_indexed','sex_indexed'] + numCol)\\\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#définition du pipeline\n",
    "transformationPipeline = Pipeline()\\\n",
    "                        .setStages([classe_indexer, sex_indexer, race_indexer, relationship_indexer ,occupation_indexer, marital_status_indexer, education_indexer, workclass_indexer,  assembleur])\n",
    "\n",
    "\n",
    "#la fonction fit pour rencenser les valeurs possibles des index pour chaque variable à partir du dataframe \n",
    "fittedPipeline = transformationPipeline.fit(data)\n",
    "\n",
    "#appliquer les résultats de la fonction fit pour transformer le DataFrame en ajoutnat les nouvelles colonnes.\n",
    "#on obtien une nouvelle dataframe\n",
    "transformedTraining = fittedPipeline.transform(data)\n",
    "model_data=transformedTraining.select(\"features\",\"classe_indexed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a0c1edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData=model_data.sample(0.7,123)\n",
    "testData=model_data.sample(0.3,123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c20480d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  DataFrame[features: vector, classe_indexed: double, rawPrediction: vector, probability: vector, prediction: double]\n",
      "Confusion Matrix:\n",
      "[[6850  626]\n",
      " [ 809 1591]]\n",
      "Accuracy: 0.8526970939481168\n",
      "Classifier trained in 39.579 seconds\n"
     ]
    }
   ],
   "source": [
    "time_prediction=[]\n",
    "classifier = DecisionTreeClassifier(labelCol=\"classe_indexed\", featuresCol=\"features\")\n",
    "# add empty parameter grid \n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(classifier.maxDepth, [2,5,10,15,20,25]) # max depth parameter \n",
    "             .build())\n",
    "    \n",
    "# create evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"classe_indexed\")\n",
    "\n",
    "# create cross validation object\n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5) \n",
    "\n",
    "#model_data=model_data.sample(0.1,123)\n",
    "#(trainingData, testData) = model_data.randomSplit([0.7, 0.3])\n",
    "\n",
    "t0 = time()\n",
    "# run cross-validation, and choose the best set of parameters.\n",
    "Model = crossval.fit(trainingData)\n",
    "# make predictions on test data \n",
    "predictions = Model.transform(testData)\n",
    "tt = time() - t0\n",
    "acc = evaluator.evaluate(predictions)\n",
    "print(\"Prediction Accuracy: \", predictions)\n",
    "\n",
    "y_pred=predictions.select(\"prediction\").collect()\n",
    "y_orig=predictions.select(\"classe_indexed\").collect()\n",
    "\n",
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print('Accuracy:', evaluator.evaluate(predictions))\n",
    "print (\"Classifier trained in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74d984d",
   "metadata": {},
   "source": [
    "# 10 coeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48982dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bbab27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from time import time\n",
    "import numpy as np\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[10]\")\\\n",
    "        .appName(\"test_avec_10coeur\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "path=\"adult2.data\"\n",
    "data = spark.read.csv(path, inferSchema=True)\n",
    "new_names=[('_c0','age'),('_c1','workclass'),('_c2','fnlwgt'),('_c3','education'),('_c4','education_num'),('_c5','marital_status'),('_c6','occupation'),('_c7','relationship'),('_c8','race'),('_c9','sex'),('_c10','capital_gain'),('_c11','capital_loss'),('_c12','hours_per_week'),('_c13','native_country'),('_c14','classe')]\n",
    "for old_name,new_name in new_names:\n",
    "    data=data.withColumnRenamed(old_name,new_name)\n",
    "data=data.drop(\"native_country\")\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "workclass_indexer = StringIndexer().setInputCol(\"workclass\").setOutputCol(\"workclass_indexed\")\n",
    "education_indexer = StringIndexer().setInputCol(\"education\").setOutputCol(\"education_indexed\")\n",
    "marital_status_indexer = StringIndexer().setInputCol(\"marital_status\").setOutputCol(\"marital_status_indexed\")\n",
    "occupation_indexer = StringIndexer().setInputCol(\"occupation\").setOutputCol(\"occupation_indexed\")\n",
    "relationship_indexer = StringIndexer().setInputCol(\"relationship\").setOutputCol(\"relationship_indexed\")\n",
    "race_indexer = StringIndexer().setInputCol(\"race\").setOutputCol(\"race_indexed\")\n",
    "sex_indexer = StringIndexer().setInputCol(\"sex\").setOutputCol(\"sex_indexed\")\n",
    "classe_indexer = StringIndexer().setInputCol(\"classe\").setOutputCol(\"classe_indexed\")\n",
    "\n",
    "numCol = [\"age\",'fnlwgt', 'education_num' ]\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembleur = VectorAssembler()\\\n",
    "    .setInputCols(['workclass_indexed','education_indexed','marital_status_indexed','occupation_indexed','relationship_indexed','race_indexed','sex_indexed'] + numCol)\\\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#définition du pipeline\n",
    "transformationPipeline = Pipeline()\\\n",
    "                        .setStages([classe_indexer, sex_indexer, race_indexer, relationship_indexer ,occupation_indexer, marital_status_indexer, education_indexer, workclass_indexer,  assembleur])\n",
    "\n",
    "\n",
    "#la fonction fit pour rencenser les valeurs possibles des index pour chaque variable à partir du dataframe \n",
    "fittedPipeline = transformationPipeline.fit(data)\n",
    "\n",
    "#appliquer les résultats de la fonction fit pour transformer le DataFrame en ajoutnat les nouvelles colonnes.\n",
    "#on obtien une nouvelle dataframe\n",
    "transformedTraining = fittedPipeline.transform(data)\n",
    "model_data=transformedTraining.select(\"features\",\"classe_indexed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfb21918",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData=model_data.sample(0.7,123)\n",
    "testData=model_data.sample(0.3,123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d1f5733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  DataFrame[features: vector, classe_indexed: double, rawPrediction: vector, probability: vector, prediction: double]\n",
      "Confusion Matrix:\n",
      "[[6850  626]\n",
      " [ 809 1591]]\n",
      "Accuracy: 0.8526970939481168\n",
      "Classifier trained in 39.465 seconds\n"
     ]
    }
   ],
   "source": [
    "time_prediction=[]\n",
    "classifier = DecisionTreeClassifier(labelCol=\"classe_indexed\", featuresCol=\"features\")\n",
    "# add empty parameter grid \n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(classifier.maxDepth, [2,5,10,15,20,25]) # max depth parameter \n",
    "             .build())\n",
    "    \n",
    "# create evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"classe_indexed\")\n",
    "\n",
    "# create cross validation object\n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5) \n",
    "\n",
    "#model_data=model_data.sample(0.1,123)\n",
    "#(trainingData, testData) = model_data.randomSplit([0.7, 0.3])\n",
    "\n",
    "t0 = time()\n",
    "# run cross-validation, and choose the best set of parameters.\n",
    "Model = crossval.fit(trainingData)\n",
    "# make predictions on test data \n",
    "predictions = Model.transform(testData)\n",
    "tt = time() - t0\n",
    "acc = evaluator.evaluate(predictions)\n",
    "print(\"Prediction Accuracy: \", predictions)\n",
    "\n",
    "y_pred=predictions.select(\"prediction\").collect()\n",
    "y_orig=predictions.select(\"classe_indexed\").collect()\n",
    "\n",
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print('Accuracy:', evaluator.evaluate(predictions))\n",
    "print (\"Classifier trained in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5fda43",
   "metadata": {},
   "source": [
    "# 12 coeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8628601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5dceea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from time import time\n",
    "import numpy as np\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[12]\")\\\n",
    "        .appName(\"test_avec_12coeur\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "path=\"adult2.data\"\n",
    "data = spark.read.csv(path, inferSchema=True)\n",
    "new_names=[('_c0','age'),('_c1','workclass'),('_c2','fnlwgt'),('_c3','education'),('_c4','education_num'),('_c5','marital_status'),('_c6','occupation'),('_c7','relationship'),('_c8','race'),('_c9','sex'),('_c10','capital_gain'),('_c11','capital_loss'),('_c12','hours_per_week'),('_c13','native_country'),('_c14','classe')]\n",
    "for old_name,new_name in new_names:\n",
    "    data=data.withColumnRenamed(old_name,new_name)\n",
    "data=data.drop(\"native_country\")\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "workclass_indexer = StringIndexer().setInputCol(\"workclass\").setOutputCol(\"workclass_indexed\")\n",
    "education_indexer = StringIndexer().setInputCol(\"education\").setOutputCol(\"education_indexed\")\n",
    "marital_status_indexer = StringIndexer().setInputCol(\"marital_status\").setOutputCol(\"marital_status_indexed\")\n",
    "occupation_indexer = StringIndexer().setInputCol(\"occupation\").setOutputCol(\"occupation_indexed\")\n",
    "relationship_indexer = StringIndexer().setInputCol(\"relationship\").setOutputCol(\"relationship_indexed\")\n",
    "race_indexer = StringIndexer().setInputCol(\"race\").setOutputCol(\"race_indexed\")\n",
    "sex_indexer = StringIndexer().setInputCol(\"sex\").setOutputCol(\"sex_indexed\")\n",
    "classe_indexer = StringIndexer().setInputCol(\"classe\").setOutputCol(\"classe_indexed\")\n",
    "\n",
    "numCol = [\"age\",'fnlwgt', 'education_num' ]\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembleur = VectorAssembler()\\\n",
    "    .setInputCols(['workclass_indexed','education_indexed','marital_status_indexed','occupation_indexed','relationship_indexed','race_indexed','sex_indexed'] + numCol)\\\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#définition du pipeline\n",
    "transformationPipeline = Pipeline()\\\n",
    "                        .setStages([classe_indexer, sex_indexer, race_indexer, relationship_indexer ,occupation_indexer, marital_status_indexer, education_indexer, workclass_indexer,  assembleur])\n",
    "\n",
    "\n",
    "#la fonction fit pour rencenser les valeurs possibles des index pour chaque variable à partir du dataframe \n",
    "fittedPipeline = transformationPipeline.fit(data)\n",
    "\n",
    "#appliquer les résultats de la fonction fit pour transformer le DataFrame en ajoutnat les nouvelles colonnes.\n",
    "#on obtien une nouvelle dataframe\n",
    "transformedTraining = fittedPipeline.transform(data)\n",
    "model_data=transformedTraining.select(\"features\",\"classe_indexed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b704b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData=model_data.sample(0.7,123)\n",
    "testData=model_data.sample(0.3,123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "675184a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  DataFrame[features: vector, classe_indexed: double, rawPrediction: vector, probability: vector, prediction: double]\n",
      "Confusion Matrix:\n",
      "[[6850  626]\n",
      " [ 809 1591]]\n",
      "Accuracy: 0.8526970939481168\n",
      "Classifier trained in 39.424 seconds\n"
     ]
    }
   ],
   "source": [
    "time_prediction=[]\n",
    "classifier = DecisionTreeClassifier(labelCol=\"classe_indexed\", featuresCol=\"features\")\n",
    "# add empty parameter grid \n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(classifier.maxDepth, [2,5,10,15,20,25]) # max depth parameter \n",
    "             .build())\n",
    "    \n",
    "# create evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"classe_indexed\")\n",
    "\n",
    "# create cross validation object\n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5) \n",
    "\n",
    "#model_data=model_data.sample(0.1,123)\n",
    "#(trainingData, testData) = model_data.randomSplit([0.7, 0.3])\n",
    "\n",
    "t0 = time()\n",
    "# run cross-validation, and choose the best set of parameters.\n",
    "Model = crossval.fit(trainingData)\n",
    "# make predictions on test data \n",
    "predictions = Model.transform(testData)\n",
    "tt = time() - t0\n",
    "acc = evaluator.evaluate(predictions)\n",
    "print(\"Prediction Accuracy: \", predictions)\n",
    "\n",
    "y_pred=predictions.select(\"prediction\").collect()\n",
    "y_orig=predictions.select(\"classe_indexed\").collect()\n",
    "\n",
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print('Accuracy:', evaluator.evaluate(predictions))\n",
    "print (\"Classifier trained in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08389268",
   "metadata": {},
   "source": [
    "# 14 coeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c75ef4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b9284da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from time import time\n",
    "import numpy as np\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[14]\")\\\n",
    "        .appName(\"test_avec_14coeur\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "path=\"adult2.data\"\n",
    "data = spark.read.csv(path, inferSchema=True)\n",
    "new_names=[('_c0','age'),('_c1','workclass'),('_c2','fnlwgt'),('_c3','education'),('_c4','education_num'),('_c5','marital_status'),('_c6','occupation'),('_c7','relationship'),('_c8','race'),('_c9','sex'),('_c10','capital_gain'),('_c11','capital_loss'),('_c12','hours_per_week'),('_c13','native_country'),('_c14','classe')]\n",
    "for old_name,new_name in new_names:\n",
    "    data=data.withColumnRenamed(old_name,new_name)\n",
    "data=data.drop(\"native_country\")\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "workclass_indexer = StringIndexer().setInputCol(\"workclass\").setOutputCol(\"workclass_indexed\")\n",
    "education_indexer = StringIndexer().setInputCol(\"education\").setOutputCol(\"education_indexed\")\n",
    "marital_status_indexer = StringIndexer().setInputCol(\"marital_status\").setOutputCol(\"marital_status_indexed\")\n",
    "occupation_indexer = StringIndexer().setInputCol(\"occupation\").setOutputCol(\"occupation_indexed\")\n",
    "relationship_indexer = StringIndexer().setInputCol(\"relationship\").setOutputCol(\"relationship_indexed\")\n",
    "race_indexer = StringIndexer().setInputCol(\"race\").setOutputCol(\"race_indexed\")\n",
    "sex_indexer = StringIndexer().setInputCol(\"sex\").setOutputCol(\"sex_indexed\")\n",
    "classe_indexer = StringIndexer().setInputCol(\"classe\").setOutputCol(\"classe_indexed\")\n",
    "\n",
    "numCol = [\"age\",'fnlwgt', 'education_num' ]\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembleur = VectorAssembler()\\\n",
    "    .setInputCols(['workclass_indexed','education_indexed','marital_status_indexed','occupation_indexed','relationship_indexed','race_indexed','sex_indexed'] + numCol)\\\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#définition du pipeline\n",
    "transformationPipeline = Pipeline()\\\n",
    "                        .setStages([classe_indexer, sex_indexer, race_indexer, relationship_indexer ,occupation_indexer, marital_status_indexer, education_indexer, workclass_indexer,  assembleur])\n",
    "\n",
    "\n",
    "#la fonction fit pour rencenser les valeurs possibles des index pour chaque variable à partir du dataframe \n",
    "fittedPipeline = transformationPipeline.fit(data)\n",
    "\n",
    "#appliquer les résultats de la fonction fit pour transformer le DataFrame en ajoutnat les nouvelles colonnes.\n",
    "#on obtien une nouvelle dataframe\n",
    "transformedTraining = fittedPipeline.transform(data)\n",
    "model_data=transformedTraining.select(\"features\",\"classe_indexed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f4249ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData=model_data.sample(0.7,123)\n",
    "testData=model_data.sample(0.3,123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "646f27fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  DataFrame[features: vector, classe_indexed: double, rawPrediction: vector, probability: vector, prediction: double]\n",
      "Confusion Matrix:\n",
      "[[6850  626]\n",
      " [ 809 1591]]\n",
      "Accuracy: 0.8526970939481168\n",
      "Classifier trained in 39.434 seconds\n"
     ]
    }
   ],
   "source": [
    "time_prediction=[]\n",
    "classifier = DecisionTreeClassifier(labelCol=\"classe_indexed\", featuresCol=\"features\")\n",
    "# add empty parameter grid \n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(classifier.maxDepth, [2,5,10,15,20,25]) # max depth parameter \n",
    "             .build())\n",
    "    \n",
    "# create evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"classe_indexed\")\n",
    "\n",
    "# create cross validation object\n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5) \n",
    "\n",
    "#model_data=model_data.sample(0.1,123)\n",
    "#(trainingData, testData) = model_data.randomSplit([0.7, 0.3])\n",
    "\n",
    "t0 = time()\n",
    "# run cross-validation, and choose the best set of parameters.\n",
    "Model = crossval.fit(trainingData)\n",
    "# make predictions on test data \n",
    "predictions = Model.transform(testData)\n",
    "tt = time() - t0\n",
    "acc = evaluator.evaluate(predictions)\n",
    "print(\"Prediction Accuracy: \", predictions)\n",
    "\n",
    "y_pred=predictions.select(\"prediction\").collect()\n",
    "y_orig=predictions.select(\"classe_indexed\").collect()\n",
    "\n",
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print('Accuracy:', evaluator.evaluate(predictions))\n",
    "print (\"Classifier trained in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a6e072",
   "metadata": {},
   "source": [
    "# 16 coeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12df65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5c58bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from time import time\n",
    "import numpy as np\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[16]\")\\\n",
    "        .appName(\"test_avec_14coeur\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "path=\"adult2.data\"\n",
    "data = spark.read.csv(path, inferSchema=True)\n",
    "new_names=[('_c0','age'),('_c1','workclass'),('_c2','fnlwgt'),('_c3','education'),('_c4','education_num'),('_c5','marital_status'),('_c6','occupation'),('_c7','relationship'),('_c8','race'),('_c9','sex'),('_c10','capital_gain'),('_c11','capital_loss'),('_c12','hours_per_week'),('_c13','native_country'),('_c14','classe')]\n",
    "for old_name,new_name in new_names:\n",
    "    data=data.withColumnRenamed(old_name,new_name)\n",
    "data=data.drop(\"native_country\")\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "workclass_indexer = StringIndexer().setInputCol(\"workclass\").setOutputCol(\"workclass_indexed\")\n",
    "education_indexer = StringIndexer().setInputCol(\"education\").setOutputCol(\"education_indexed\")\n",
    "marital_status_indexer = StringIndexer().setInputCol(\"marital_status\").setOutputCol(\"marital_status_indexed\")\n",
    "occupation_indexer = StringIndexer().setInputCol(\"occupation\").setOutputCol(\"occupation_indexed\")\n",
    "relationship_indexer = StringIndexer().setInputCol(\"relationship\").setOutputCol(\"relationship_indexed\")\n",
    "race_indexer = StringIndexer().setInputCol(\"race\").setOutputCol(\"race_indexed\")\n",
    "sex_indexer = StringIndexer().setInputCol(\"sex\").setOutputCol(\"sex_indexed\")\n",
    "classe_indexer = StringIndexer().setInputCol(\"classe\").setOutputCol(\"classe_indexed\")\n",
    "\n",
    "numCol = [\"age\",'fnlwgt', 'education_num' ]\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembleur = VectorAssembler()\\\n",
    "    .setInputCols(['workclass_indexed','education_indexed','marital_status_indexed','occupation_indexed','relationship_indexed','race_indexed','sex_indexed'] + numCol)\\\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#définition du pipeline\n",
    "transformationPipeline = Pipeline()\\\n",
    "                        .setStages([classe_indexer, sex_indexer, race_indexer, relationship_indexer ,occupation_indexer, marital_status_indexer, education_indexer, workclass_indexer,  assembleur])\n",
    "\n",
    "\n",
    "#la fonction fit pour rencenser les valeurs possibles des index pour chaque variable à partir du dataframe \n",
    "fittedPipeline = transformationPipeline.fit(data)\n",
    "\n",
    "#appliquer les résultats de la fonction fit pour transformer le DataFrame en ajoutnat les nouvelles colonnes.\n",
    "#on obtien une nouvelle dataframe\n",
    "transformedTraining = fittedPipeline.transform(data)\n",
    "model_data=transformedTraining.select(\"features\",\"classe_indexed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f63c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData=model_data.sample(0.7,123)\n",
    "testData=model_data.sample(0.3,123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8a350d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  DataFrame[features: vector, classe_indexed: double, rawPrediction: vector, probability: vector, prediction: double]\n",
      "Confusion Matrix:\n",
      "[[6850  626]\n",
      " [ 809 1591]]\n",
      "Accuracy: 0.8526970939481168\n",
      "Classifier trained in 39.56 seconds\n"
     ]
    }
   ],
   "source": [
    "time_prediction=[]\n",
    "classifier = DecisionTreeClassifier(labelCol=\"classe_indexed\", featuresCol=\"features\")\n",
    "# add empty parameter grid \n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(classifier.maxDepth, [2,5,10,15,20,25]) # max depth parameter \n",
    "             .build())\n",
    "    \n",
    "# create evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"classe_indexed\")\n",
    "\n",
    "# create cross validation object\n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5) \n",
    "\n",
    "#model_data=model_data.sample(0.1,123)\n",
    "#(trainingData, testData) = model_data.randomSplit([0.7, 0.3])\n",
    "\n",
    "t0 = time()\n",
    "# run cross-validation, and choose the best set of parameters.\n",
    "Model = crossval.fit(trainingData)\n",
    "# make predictions on test data \n",
    "predictions = Model.transform(testData)\n",
    "tt = time() - t0\n",
    "acc = evaluator.evaluate(predictions)\n",
    "print(\"Prediction Accuracy: \", predictions)\n",
    "\n",
    "y_pred=predictions.select(\"prediction\").collect()\n",
    "y_orig=predictions.select(\"classe_indexed\").collect()\n",
    "\n",
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print('Accuracy:', evaluator.evaluate(predictions))\n",
    "print (\"Classifier trained in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4d668c",
   "metadata": {},
   "source": [
    "# 18 coeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3722b8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "161bc7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from time import time\n",
    "import numpy as np\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[16]\")\\\n",
    "        .appName(\"test_avec_14coeur\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "path=\"adult2.data\"\n",
    "data = spark.read.csv(path, inferSchema=True)\n",
    "new_names=[('_c0','age'),('_c1','workclass'),('_c2','fnlwgt'),('_c3','education'),('_c4','education_num'),('_c5','marital_status'),('_c6','occupation'),('_c7','relationship'),('_c8','race'),('_c9','sex'),('_c10','capital_gain'),('_c11','capital_loss'),('_c12','hours_per_week'),('_c13','native_country'),('_c14','classe')]\n",
    "for old_name,new_name in new_names:\n",
    "    data=data.withColumnRenamed(old_name,new_name)\n",
    "data=data.drop(\"native_country\")\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "workclass_indexer = StringIndexer().setInputCol(\"workclass\").setOutputCol(\"workclass_indexed\")\n",
    "education_indexer = StringIndexer().setInputCol(\"education\").setOutputCol(\"education_indexed\")\n",
    "marital_status_indexer = StringIndexer().setInputCol(\"marital_status\").setOutputCol(\"marital_status_indexed\")\n",
    "occupation_indexer = StringIndexer().setInputCol(\"occupation\").setOutputCol(\"occupation_indexed\")\n",
    "relationship_indexer = StringIndexer().setInputCol(\"relationship\").setOutputCol(\"relationship_indexed\")\n",
    "race_indexer = StringIndexer().setInputCol(\"race\").setOutputCol(\"race_indexed\")\n",
    "sex_indexer = StringIndexer().setInputCol(\"sex\").setOutputCol(\"sex_indexed\")\n",
    "classe_indexer = StringIndexer().setInputCol(\"classe\").setOutputCol(\"classe_indexed\")\n",
    "\n",
    "numCol = [\"age\",'fnlwgt', 'education_num' ]\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembleur = VectorAssembler()\\\n",
    "    .setInputCols(['workclass_indexed','education_indexed','marital_status_indexed','occupation_indexed','relationship_indexed','race_indexed','sex_indexed'] + numCol)\\\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#définition du pipeline\n",
    "transformationPipeline = Pipeline()\\\n",
    "                        .setStages([classe_indexer, sex_indexer, race_indexer, relationship_indexer ,occupation_indexer, marital_status_indexer, education_indexer, workclass_indexer,  assembleur])\n",
    "\n",
    "\n",
    "#la fonction fit pour rencenser les valeurs possibles des index pour chaque variable à partir du dataframe \n",
    "fittedPipeline = transformationPipeline.fit(data)\n",
    "\n",
    "#appliquer les résultats de la fonction fit pour transformer le DataFrame en ajoutnat les nouvelles colonnes.\n",
    "#on obtien une nouvelle dataframe\n",
    "transformedTraining = fittedPipeline.transform(data)\n",
    "model_data=transformedTraining.select(\"features\",\"classe_indexed\")\n",
    "trainingData=model_data.sample(0.7,123)\n",
    "testData=model_data.sample(0.3,123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77b4d17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  DataFrame[features: vector, classe_indexed: double, rawPrediction: vector, probability: vector, prediction: double]\n",
      "Confusion Matrix:\n",
      "[[6850  626]\n",
      " [ 809 1591]]\n",
      "Accuracy: 0.8526970939481168\n",
      "Classifier trained in 39.519 seconds\n"
     ]
    }
   ],
   "source": [
    "time_prediction=[]\n",
    "classifier = DecisionTreeClassifier(labelCol=\"classe_indexed\", featuresCol=\"features\")\n",
    "# add empty parameter grid \n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(classifier.maxDepth, [2,5,10,15,20,25]) # max depth parameter \n",
    "             .build())\n",
    "    \n",
    "# create evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"classe_indexed\")\n",
    "\n",
    "# create cross validation object\n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5) \n",
    "\n",
    "#model_data=model_data.sample(0.1,123)\n",
    "#(trainingData, testData) = model_data.randomSplit([0.7, 0.3])\n",
    "\n",
    "t0 = time()\n",
    "# run cross-validation, and choose the best set of parameters.\n",
    "Model = crossval.fit(trainingData)\n",
    "# make predictions on test data \n",
    "predictions = Model.transform(testData)\n",
    "tt = time() - t0\n",
    "acc = evaluator.evaluate(predictions)\n",
    "print(\"Prediction Accuracy: \", predictions)\n",
    "\n",
    "y_pred=predictions.select(\"prediction\").collect()\n",
    "y_orig=predictions.select(\"classe_indexed\").collect()\n",
    "\n",
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print('Accuracy:', evaluator.evaluate(predictions))\n",
    "print (\"Classifier trained in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae4ea08",
   "metadata": {},
   "source": [
    "# 20 coeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56e657d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a352124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from time import time\n",
    "import numpy as np\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[16]\")\\\n",
    "        .appName(\"test_avec_14coeur\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "path=\"adult2.data\"\n",
    "data = spark.read.csv(path, inferSchema=True)\n",
    "new_names=[('_c0','age'),('_c1','workclass'),('_c2','fnlwgt'),('_c3','education'),('_c4','education_num'),('_c5','marital_status'),('_c6','occupation'),('_c7','relationship'),('_c8','race'),('_c9','sex'),('_c10','capital_gain'),('_c11','capital_loss'),('_c12','hours_per_week'),('_c13','native_country'),('_c14','classe')]\n",
    "for old_name,new_name in new_names:\n",
    "    data=data.withColumnRenamed(old_name,new_name)\n",
    "data=data.drop(\"native_country\")\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "workclass_indexer = StringIndexer().setInputCol(\"workclass\").setOutputCol(\"workclass_indexed\")\n",
    "education_indexer = StringIndexer().setInputCol(\"education\").setOutputCol(\"education_indexed\")\n",
    "marital_status_indexer = StringIndexer().setInputCol(\"marital_status\").setOutputCol(\"marital_status_indexed\")\n",
    "occupation_indexer = StringIndexer().setInputCol(\"occupation\").setOutputCol(\"occupation_indexed\")\n",
    "relationship_indexer = StringIndexer().setInputCol(\"relationship\").setOutputCol(\"relationship_indexed\")\n",
    "race_indexer = StringIndexer().setInputCol(\"race\").setOutputCol(\"race_indexed\")\n",
    "sex_indexer = StringIndexer().setInputCol(\"sex\").setOutputCol(\"sex_indexed\")\n",
    "classe_indexer = StringIndexer().setInputCol(\"classe\").setOutputCol(\"classe_indexed\")\n",
    "\n",
    "numCol = [\"age\",'fnlwgt', 'education_num' ]\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembleur = VectorAssembler()\\\n",
    "    .setInputCols(['workclass_indexed','education_indexed','marital_status_indexed','occupation_indexed','relationship_indexed','race_indexed','sex_indexed'] + numCol)\\\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#définition du pipeline\n",
    "transformationPipeline = Pipeline()\\\n",
    "                        .setStages([classe_indexer, sex_indexer, race_indexer, relationship_indexer ,occupation_indexer, marital_status_indexer, education_indexer, workclass_indexer,  assembleur])\n",
    "\n",
    "\n",
    "#la fonction fit pour rencenser les valeurs possibles des index pour chaque variable à partir du dataframe \n",
    "fittedPipeline = transformationPipeline.fit(data)\n",
    "\n",
    "#appliquer les résultats de la fonction fit pour transformer le DataFrame en ajoutnat les nouvelles colonnes.\n",
    "#on obtien une nouvelle dataframe\n",
    "transformedTraining = fittedPipeline.transform(data)\n",
    "model_data=transformedTraining.select(\"features\",\"classe_indexed\")\n",
    "trainingData=model_data.sample(0.7,123)\n",
    "testData=model_data.sample(0.3,123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fcde84cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  DataFrame[features: vector, classe_indexed: double, rawPrediction: vector, probability: vector, prediction: double]\n",
      "Confusion Matrix:\n",
      "[[6850  626]\n",
      " [ 809 1591]]\n",
      "Accuracy: 0.8526970939481168\n",
      "Classifier trained in 39.462 seconds\n"
     ]
    }
   ],
   "source": [
    "time_prediction=[]\n",
    "classifier = DecisionTreeClassifier(labelCol=\"classe_indexed\", featuresCol=\"features\")\n",
    "# add empty parameter grid \n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(classifier.maxDepth, [2,5,10,15,20,25]) # max depth parameter \n",
    "             .build())\n",
    "    \n",
    "# create evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"classe_indexed\")\n",
    "\n",
    "# create cross validation object\n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5) \n",
    "\n",
    "#model_data=model_data.sample(0.1,123)\n",
    "#(trainingData, testData) = model_data.randomSplit([0.7, 0.3])\n",
    "\n",
    "t0 = time()\n",
    "# run cross-validation, and choose the best set of parameters.\n",
    "Model = crossval.fit(trainingData)\n",
    "# make predictions on test data \n",
    "predictions = Model.transform(testData)\n",
    "tt = time() - t0\n",
    "acc = evaluator.evaluate(predictions)\n",
    "print(\"Prediction Accuracy: \", predictions)\n",
    "\n",
    "y_pred=predictions.select(\"prediction\").collect()\n",
    "y_orig=predictions.select(\"classe_indexed\").collect()\n",
    "\n",
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print('Accuracy:', evaluator.evaluate(predictions))\n",
    "print (\"Classifier trained in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0933fd",
   "metadata": {},
   "source": [
    "# sklearn vs mllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6649235b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                Int64\n",
       "workclass         string\n",
       "fnlwgt             Int64\n",
       "education         string\n",
       "education_num      Int64\n",
       "marital_status    string\n",
       "occupation        string\n",
       "relationship      string\n",
       "race              string\n",
       "sex               string\n",
       "capital_gain       Int64\n",
       "capital_loss       Int64\n",
       "house_per_week     Int64\n",
       "native_country    string\n",
       "classe            string\n",
       "dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd \n",
    "\n",
    "data=pd.read_csv(\"adult1.data\", nrows=2565805,header=None)\n",
    "data=data.rename(columns={0: \"age\", 1: \"workclass\", 2: \"fnlwgt\",3: \"education\", 4: \"education_num\", 5: \"marital_status\",6: \"occupation\", 7: \"relationship\", 8: \"race\",9: \"sex\",10: \"capital_gain\", 11: \"capital_loss\",12: \"house_per_week\", 13: \"native_country\", 14:\"classe\"})\n",
    "data=data.convert_dtypes()\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03a8a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "data['workclass']= le.fit_transform(data['workclass']) \n",
    "data['education']= le.fit_transform(data['education']) \n",
    "data['marital_status']= le.fit_transform(data['marital_status']) \n",
    "data['occupation']= le.fit_transform(data['occupation']) \n",
    "data['relationship']= le.fit_transform(data['relationship']) \n",
    "data['race']= le.fit_transform(data['race']) \n",
    "data['classe']= le.fit_transform(data['classe'])  \n",
    "data['sex']= le.fit_transform(data['sex']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dcdb307",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(columns=[\"native_country\"])\n",
    "y=data[\"classe\"]\n",
    "x=data.drop(columns=[\"classe\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e14519a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthu\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained in 136.432 seconds\n",
      "scrore 0.9826308995478046\n"
     ]
    }
   ],
   "source": [
    "#model=DecisionTreeClassifier()\n",
    "#model.fit(x,y)\n",
    "#model.score(x,y)\n",
    "#model.predict(x)\n",
    "from time import time\n",
    "t0 = time()\n",
    "test=[]\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.3, random_state=0)\n",
    "for depth in [2,5,10,15,20,25]:\n",
    "    model= DecisionTreeClassifier(max_depth=depth)\n",
    "    score=cross_val_score(model,X_train,y_train,cv=5, scoring='accuracy').mean()\n",
    "    test.append(test)\n",
    "tt = time() - t0\n",
    "print (\"Classifier trained in {} seconds\".format(round(tt,3)))\n",
    "print(\"scrore\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8092349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77745308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from time import time\n",
    "import numpy as np\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName(\"test_avec_14coeur\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "path=\"adult2.data\"\n",
    "data = spark.read.csv(path, inferSchema=True)\n",
    "new_names=[('_c0','age'),('_c1','workclass'),('_c2','fnlwgt'),('_c3','education'),('_c4','education_num'),('_c5','marital_status'),('_c6','occupation'),('_c7','relationship'),('_c8','race'),('_c9','sex'),('_c10','capital_gain'),('_c11','capital_loss'),('_c12','hours_per_week'),('_c13','native_country'),('_c14','classe')]\n",
    "for old_name,new_name in new_names:\n",
    "    data=data.withColumnRenamed(old_name,new_name)\n",
    "data=data.drop(\"native_country\")\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "workclass_indexer = StringIndexer().setInputCol(\"workclass\").setOutputCol(\"workclass_indexed\")\n",
    "education_indexer = StringIndexer().setInputCol(\"education\").setOutputCol(\"education_indexed\")\n",
    "marital_status_indexer = StringIndexer().setInputCol(\"marital_status\").setOutputCol(\"marital_status_indexed\")\n",
    "occupation_indexer = StringIndexer().setInputCol(\"occupation\").setOutputCol(\"occupation_indexed\")\n",
    "relationship_indexer = StringIndexer().setInputCol(\"relationship\").setOutputCol(\"relationship_indexed\")\n",
    "race_indexer = StringIndexer().setInputCol(\"race\").setOutputCol(\"race_indexed\")\n",
    "sex_indexer = StringIndexer().setInputCol(\"sex\").setOutputCol(\"sex_indexed\")\n",
    "classe_indexer = StringIndexer().setInputCol(\"classe\").setOutputCol(\"classe_indexed\")\n",
    "\n",
    "numCol = [\"age\",'fnlwgt', 'education_num' ]\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembleur = VectorAssembler()\\\n",
    "    .setInputCols(['workclass_indexed','education_indexed','marital_status_indexed','occupation_indexed','relationship_indexed','race_indexed','sex_indexed'] + numCol)\\\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#définition du pipeline\n",
    "transformationPipeline = Pipeline()\\\n",
    "                        .setStages([classe_indexer, sex_indexer, race_indexer, relationship_indexer ,occupation_indexer, marital_status_indexer, education_indexer, workclass_indexer,  assembleur])\n",
    "\n",
    "\n",
    "#la fonction fit pour rencenser les valeurs possibles des index pour chaque variable à partir du dataframe \n",
    "fittedPipeline = transformationPipeline.fit(data)\n",
    "\n",
    "#appliquer les résultats de la fonction fit pour transformer le DataFrame en ajoutnat les nouvelles colonnes.\n",
    "#on obtien une nouvelle dataframe\n",
    "transformedTraining = fittedPipeline.transform(data)\n",
    "model_data=transformedTraining.select(\"features\",\"classe_indexed\")\n",
    "trainingData=model_data.sample(0.7,123)\n",
    "testData=model_data.sample(0.3,123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a034260b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  DataFrame[features: vector, classe_indexed: double, rawPrediction: vector, probability: vector, prediction: double]\n",
      "Confusion Matrix:\n",
      "[[6996  459]\n",
      " [ 979 1426]]\n",
      "Accuracy: 0.8477767955604787\n",
      "Classifier trained in 41.315 seconds\n"
     ]
    }
   ],
   "source": [
    "time_prediction=[]\n",
    "classifier = DecisionTreeClassifier(labelCol=\"classe_indexed\", featuresCol=\"features\")\n",
    "# add empty parameter grid \n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(classifier.maxDepth, [2,5,10,15,20,25]) # max depth parameter \n",
    "             .build())\n",
    "    \n",
    "# create evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"classe_indexed\")\n",
    "\n",
    "# create cross validation object\n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5) \n",
    "\n",
    "#model_data=model_data.sample(0.1,123)\n",
    "#(trainingData, testData) = model_data.randomSplit([0.7, 0.3])\n",
    "\n",
    "t0 = time()\n",
    "# run cross-validation, and choose the best set of parameters.\n",
    "Model = crossval.fit(trainingData)\n",
    "tt = time() - t0\n",
    "# make predictions on test data \n",
    "predictions = Model.transform(testData)\n",
    "\n",
    "acc = evaluator.evaluate(predictions)\n",
    "print(\"Prediction Accuracy: \", predictions)\n",
    "\n",
    "y_pred=predictions.select(\"prediction\").collect()\n",
    "y_orig=predictions.select(\"classe_indexed\").collect()\n",
    "\n",
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print('Accuracy:', evaluator.evaluate(predictions))\n",
    "print (\"Classifier trained in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e78a76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
